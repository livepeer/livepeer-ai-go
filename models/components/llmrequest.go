// Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT.

package components

import (
	"github.com/livepeer/livepeer-ai-go/internal/utils"
)

type LLMRequest struct {
	Messages    []LLMMessage `json:"messages"`
	Model       *string      `default:"" json:"model"`
	Temperature *float64     `default:"0.7" json:"temperature"`
	MaxTokens   *int64       `default:"256" json:"max_tokens"`
	TopP        *float64     `default:"1" json:"top_p"`
	TopK        *int64       `default:"-1" json:"top_k"`
	Stream      *bool        `default:"false" json:"stream"`
}

func (l LLMRequest) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(l, "", false)
}

func (l *LLMRequest) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &l, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *LLMRequest) GetMessages() []LLMMessage {
	if o == nil {
		return []LLMMessage{}
	}
	return o.Messages
}

func (o *LLMRequest) GetModel() *string {
	if o == nil {
		return nil
	}
	return o.Model
}

func (o *LLMRequest) GetTemperature() *float64 {
	if o == nil {
		return nil
	}
	return o.Temperature
}

func (o *LLMRequest) GetMaxTokens() *int64 {
	if o == nil {
		return nil
	}
	return o.MaxTokens
}

func (o *LLMRequest) GetTopP() *float64 {
	if o == nil {
		return nil
	}
	return o.TopP
}

func (o *LLMRequest) GetTopK() *int64 {
	if o == nil {
		return nil
	}
	return o.TopK
}

func (o *LLMRequest) GetStream() *bool {
	if o == nil {
		return nil
	}
	return o.Stream
}
